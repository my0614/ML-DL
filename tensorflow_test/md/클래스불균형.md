# 클래스 불균형
각 클래스 데이터 양의 차이가 날 경우, 클래스 불균형이 일어날 수 있다. 가장 큰 원인은 각 클래스의 데이터양이 차이가 많이 날 경우

## Weight balancing
training set의 각 데이터에서 loss를 계산할 때 특정 클래스의 loss값을 더 많이 갖도록 하는 방법이다.
클래스의 비율이 좋지 않은 클래스를 골라서 loss의 값을 올려준다.

비율에 따라서 가중치를 두는 방법이 있는데, 데이터 양이 작은 클래스를 전체 loss에 동일하게도 할 수 있다.

Focal loss를 사용하는 방법이 있다. 성능이 잘 안나오는 클래스를 집중적으로 학습을 시켜주면 된다. loss를 계산해서 backpropagation을 통해서 weight의 값을 업데이트함으로써, 전반적이니 모델의 정확도를 높이고자 한다.

- Focal loss의 특징은 weight를 주지 않고 성능이 좋은 클래스의 down-weighting을 한다. Focal loss는 loss function과 loss parameter를 사용하여 구현이 가능하다. 

## Over and under sampling
데이터의 양이 적은 클래스의 데이터를 undersampling과 oversampling 할 수 있다.

Undersampling은 데이터가 많은 것을 일부만 선택하고 적은것을 최대한 많은 데이터를 사용하는 방법입니다. 

Oversampling은 데이터의 양이 적은 클래스를 복사하여 데이터의 양이 많은 클래스의 개수만큼 데이터를 만들어 주는 방법입니다. 똑같은 데이터를 그대로 복사하는 것이기 때문에 새로운 데이터는 기존 데이터와 같은 성질을 가지게 된다.

