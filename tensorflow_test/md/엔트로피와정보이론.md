# 엔트로피와 정보이론
## 정보이론
정보량을 나타내기 위해서 엔트로피라는 단위를 사용한다.
불확실한 정보를 숫자로 정량화하려는 노력이자 하나의 도구이다.

엔트로피는 확률의 역수에 로그를 취한 값이다. 확률에 역수를 취해주는 이유는
확률이 높은 사건일수록 정보량이 적다고 판단하기 때문이다.
- 엔트로피의 기대값은 엔트로피에 확률을 곱해주는 값이다. 
엔트로피가 높으면 불확실성도 높아지기에, 엔트로피를 줄이면 불확실성도 낮아지고 의미있는 정보를 얻을 수 있다.