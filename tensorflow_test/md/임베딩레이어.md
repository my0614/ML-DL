# 임베딩레이어
## 임베딩레이어란?
자연어를 수치화된 정보로 바꾸기 위한 레이어입니다.
- 자연어는 시간의 흐름에 따라 정보가 연속적으로 이어지는 시퀸스 데이터입니다. 자연어도 정보를 가지고 나눌 수 있습니다.

단어나 n-gram으로도 많이 나뉘고, 딥러닝이 발달한 이후로는 문자단위의 자연어처리가 많이 사용되고 있습니다.

- 단어를 수치화된 데이터로 변환을 해서 원-핫코딩을 하여 단어를 구분하고 저장할 수 있습니다.

하지만 사용하는 원-핫코딩 방식의 단점은 사용하는 메모리의 양에 비해 너무 적은 정보량을 표현하는것입니다.
사이즈별로 모두 있어야하고, 없는곳은 0으로 채워줘야 하기 때문이다.

원-핫코딩방식과 다르게 임베딩레이어는 한정된 길이의 벡터로 자연어의 구성 단위인 자소, 문자, 단어, n-gram등을 표현할 수 있습니다.

임베딩레이어를 할때 실수로 값을 정해주었습니다. 실수로 하게 되면 무한대의 단어를 표현할 수 있습니다. 그리고 임베딩의 차원 수를 늘리면 임베딩의 품질이 좋아집니다. 평균적인 임베딩의 차원수는 200~500정도입니다.

### UNK
단어의 수가 매우 많기 때문에, 크기를 지정해 놓을수 없고, 정수 인덱스로 저장되지 않는 값을 위한 공간을 따로 만들어 놓습니다. 이것을 UNK라고 합니다.

### 임베딩 레이어 종류
- Word2Vec
- GloVe
- FastText
- ELMo
